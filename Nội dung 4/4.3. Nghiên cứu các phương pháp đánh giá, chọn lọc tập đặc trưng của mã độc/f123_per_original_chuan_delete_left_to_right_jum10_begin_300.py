# -*- coding: utf-8 -*-
"""F123_per_original chuan delete left to right jum10_begin_300.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13RlD0rtKImscRoWW3TjJ-uVNTBVSQoC7
"""

import pandas as pd
import csv 
import keras
from keras.layers import Input
from keras.models import Model
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.convolutional import Conv2D
from keras.layers.pooling import MaxPooling2D
from keras.layers.merge import concatenate
from tensorflow.keras.utils import to_categorical
from keras.callbacks import TensorBoard, Callback
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
import time
from google.colab import drive
drive.mount('/content/drive')

np.set_printoptions(precision=3, suppress=True)

PER_range = 1
PER_range_raw = PER_range

API_range = 877
API_range_raw = API_range

loopcount = 299
Acc_test = 0   #lưu giá trị trung bình

train_df = pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/Chạy code Feature selection 05052022/Dữ liệu mới đã đánh trọng số/AMD_Benign/F3_AMD_Bening_sort/Train/train-0.csv")#, nrows=10000)
val_df = pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/Chạy code Feature selection 05052022/Dữ liệu mới đã đánh trọng số/AMD_Benign/F3_AMD_Bening_sort/File/file-0.csv")
test_df = pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/Chạy code Feature selection 05052022/Dữ liệu mới đã đánh trọng số/AMD_Benign/F3_AMD_Bening_sort/File/file-1.csv")

print(train_df)

train_df = train_df.iloc[:,:877]
nhan_train = train_df.iloc[:,0]
val_df = val_df.iloc[:,:877]
nhan_val =  val_df.iloc[:,0]
test_df = test_df.iloc[:,:877]
nhan_test = test_df.iloc[:,0]
print("train_df ban đầu: ", train_df)
print("nhan_train_df ban đầu: ", nhan_train)

class Metrics(Callback):
    def __init__(self, x, y):
        self.x = x
        self.y = y if (y.ndim == 1 or y.shape[1] == 1) else np.argmax(y, axis=1)
        self.reports = []

    def on_epoch_end(self, epoch, logs={}):
        y_hat = np.asarray(self.model.predict(self.x))
        y_hat = np.where(y_hat > 0.5, 1, 0) if (y_hat.ndim == 1 or y_hat.shape[1] == 1)  else np.argmax(y_hat, axis=1)
        report = classification_report(self.y,y_hat,output_dict=True)
        self.reports.append(report)
        return
   
    # Utility method
    def get(self, metrics, of_class):
        return [report[str(of_class)][metrics] for report in self.reports]

'''

!WARNING: Khi chạy đoạn này, file cũ sẽ mất hết và thay vào 1 file csv raw chỉ có tên cột. 

'''
def createRawCsv():
  columns = [
           "Lần lặp",
           "Thời gian huấn luyện",
            "Acc",
            "Precision",
            "Recall",
            "F1_score" 
  ]  
  delimiter = "|"

  saveName = "/content/drive/MyDrive/Colab Notebooks/Chạy code Feature selection 05052022/Mô hình học máy học sâu/F3_Amd_Benign_sort_chuan_left_to_right_per_begin300.csv"
  
  try:
    file = open(saveName, "w")
    file.write(delimiter.join(columns) + "\n")
    file.close()
    print("Da tao csv")
  except Exception as exc:
    print (exc)

createRawCsv()

import pathlib
pathlib.Path("/content/drive/MyDrive/Colab Notebooks/Chạy code Feature selection 05052022/Mô hình học máy học sâu/F3_Amd_Benign_sort_chuan_left_to_right_per_begin300.csv").parent.resolve()

'''


Gọi hàm này khi đã có file raw CSV bên trên, nếu không thì sẽ tạo file csv không có tên cột!

Ghi luôn sau mỗi vòng lặp

'''

def writeDataToCSV(number, thoiGian, acc, precision, recall, f1):
  path = "/content/drive/MyDrive/Colab Notebooks/Chạy code Feature selection 05052022/Mô hình học máy học sâu/F3_Amd_Benign_sort_chuan_left_to_right_per_begin300.csv"
  delimiter = "|"


  #loop_range = len(acc_list) - 1
  try:
    file = open(path, "a")
    
    file.write(number)
    file.write(delimiter)

    file.write(thoiGian)
    file.write(delimiter)

    file.write(acc)
    file.write(delimiter)

    file.write(precision)
    file.write(delimiter)

    file.write(recall)
    file.write(delimiter)

    file.write(f1)
    #file.write(delimiter)



    file.write("\n")

    file.close()
  except Exception as exc:
    print(exc)




#writeDataToCSV("1", "2", "3", "4", "5", "6", "7")
# test ok

def readData(JMP_STEP):

	global loopcount, API_range#, PER_range
	global Acc_test
	PER_range_use = PER_range
	#API_range_use = API_range

	PER_range_use += JMP_STEP * loopcount
	#API_range_use += JMP_STEP * loopcount
	Acc_test = 0
	loopcount += 10
	try:
		print("vào try")
		train_df_2 = train_df.drop([str(i) for i in range(PER_range, PER_range_use)], axis = 1)
		#train_df_2 = train_df_2.drop([str(i) for i in range(API_range,API_range_use)], axis = 1)
		print("traindf2: ",train_df_2)
		val_df_2 = val_df.drop([str(i) for i in range(PER_range, PER_range_use)], axis = 1)
		#val_df_2 = val_df_2.drop([str(i) for i in range(API_range,API_range_use)], axis = 1)

		test_df_2 = test_df.drop([str(i) for i in range(PER_range, PER_range_use)], axis = 1)
		#test_df_2 = test_df_2.drop([str(i) for i in range(API_range,API_range_use)], axis = 1)
		#print(train_df_2)

		train_y = np.array(train_df_2.iloc[1:,0]) #lấy cột 0 là cột nhãn 
		train_x = np.array(train_df_2.iloc[1:,1:]) #lấy cột 1 làm đặc trưng
		unique, counts = np.unique(train_y, return_counts=True)
		print("LABEL TRAIN: ", len(unique))

		val_y = np.array(val_df_2.iloc[1:,0]) #lấy cột 0 là cột nhãn 
		val_x = np.array(val_df_2.iloc[1:,1:]) #lấy cột 1 làm đặc trưng
		unique_v, counts_ = np.unique(val_y, return_counts=True)
		#print("LABEL VAL: ", len(unique_v))

		test_y = np.array(test_df_2.iloc[1:,0]) #lấy cột 0 là cột nhãn 
		test_x = np.array(test_df_2.iloc[1:,1:]) #lấy cột 1 làm đặc trưng
		unique_t, counts_t = np.unique(test_y, return_counts=True)
		print("LABEL TEST: ", len(unique_t))

		SIZE = (len(train_df_2.columns) - 1)
		print("size = ", SIZE)
		BATCH_SIZE = 32
	#SIZE = API_range_use - 1 - JMP_STEP*2
		SIZE2= 1
		N_CLASSES = 228
		LR = 0.001
		N_EPOCHS = 10

		print("số lượng đặc trưng trong train: ",train_x.shape)
		print("số lượng đặc trưng trong val: ", val_x.shape)
		print("số lượng đặc trưng trong test: ", test_x.shape)

		train_x = train_x.reshape(train_x.shape[0], SIZE2, SIZE, 1)

		val_x = val_x.reshape(val_x.shape[0], SIZE2, SIZE, 1)
		test_x = test_x.reshape(test_x.shape[0], SIZE2, SIZE, 1)

		LABELS = np.unique(train_y)
		original_test_y = test_y

		train_y = to_categorical(train_y, N_CLASSES)
		val_y = to_categorical(val_y, N_CLASSES)
		test_y = to_categorical(test_y, N_CLASSES)

		print(LABELS)



		input0 = Input(shape=(SIZE2,SIZE,1))
		conv1 = Conv2D(32, kernel_size=2, activation='relu', padding="same" ,input_shape=(SIZE2, SIZE, 1))(input0)
		pool1 = MaxPooling2D((1, 2), padding = 'same')(conv1)
		conv2 = Conv2D(32, kernel_size=2, activation='relu', padding="same")(pool1)
		pool2 = MaxPooling2D((1, 2), padding = 'same')(conv2)

		conv3 = Conv2D(64, kernel_size=2, activation='relu', padding="same")(pool2)
		pool3 = MaxPooling2D((1, 2), padding = 'same')(conv3)
		'''
		#conv4 = Conv2D(64, kernel_size=2, activation='relu', padding="same")(pool3)
		#pool4 = MaxPooling2D((2, 2), padding = 'same')(conv4)
		'''
		flatten_per = Flatten()(pool3)

		hidden1 = Dense(1024, activation='relu')(flatten_per)
		output = Dense(N_CLASSES, activation='softmax')(hidden1)

		model = Model(inputs=input0, outputs=output)
		model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

		metrics_multiclass = Metrics(train_x, train_y)
		first_train = time.time()
		history = model.fit(train_x, train_y, epochs=N_EPOCHS, batch_size = BATCH_SIZE ,validation_data=(val_x, val_y), callbacks=[metrics_multiclass])
		second_train = time.time()


		acc = history.history['accuracy']
		val_acc = history.history['val_accuracy']

		loss=history.history['loss']
		val_loss=history.history['val_loss']

		epochs_range = range(N_EPOCHS)

		plt.figure(figsize=(8, 8))
		plt.subplot(1, 2, 1)
		plt.plot(epochs_range, acc, label='Training Accuracy')
		plt.plot(epochs_range, val_acc, label='Validation Accuracy')
		plt.legend(loc='lower right')
		plt.title('Training and Validation Accuracy')

		plt.subplot(1, 2, 2)
		plt.plot(epochs_range, loss, label='Training Loss')
		plt.plot(epochs_range, val_loss, label='Validation Loss')
		plt.legend(loc='upper right')
		plt.title('Training and Validation Loss')
		plt.show()
		plt.savefig('/content/drive/My Drive/test.png')
		plt.close('all')
	
		tmp = list(dict.fromkeys(original_test_y))
		y_pred1 = model.predict(test_x)
		y_pred = np.argmax(y_pred1, axis=1)

		# Print f1, precision, and recall scores
		'''
		cnf_matrix=confusion_matrix(original_test_y, y_pred, labels=tmp, normalize='true')
		plt.close('all')
		def plot_confusion_matrix(cm, classes,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
				plt.figure(figsize=(20, 20))
				plt.imshow(cm, interpolation='nearest', cmap=cmap)
				plt.title(title)
				plt.colorbar()
				tick_marks = np.arange(len(classes))
			
				plt.xticks(tick_marks, classes, rotation=45)
				plt.yticks(tick_marks, classes)

				plt.tight_layout()
				plt.ylabel('True label')
				plt.xlabel('Predicted label')

		#plt.figure()
		#plot_confusion_matrix(cnf_matrix, classes=tmp, title='Normalized confusion matrix')
		#plt.show()
		#plt.close('all')
		'''
		Acc_test = accuracy_score(original_test_y, y_pred)
		#print("số lượng đặc trưng trong train_per: ",train_x.shape)
		print("loopcount: ", loopcount)
		print("Thoi gian tính train riêng: ", round(second_train - first_train, 2), "s")   #chỉ đo thời gian huấn luyện
		#print("Acc_Acc: ", Acc_test)
		print("Acc: ",accuracy_score(original_test_y, y_pred))
		print("Precision: ",precision_score(original_test_y, y_pred , average="macro"))
		print("recall: ",recall_score(original_test_y, y_pred , average="macro"))
		print("F1_score: ", f1_score(original_test_y, y_pred , average="macro"))
# /content/drive/My Drive/


		writeDataToCSV(str(loopcount),str(round(second_train-first_train,2)),str(accuracy_score(original_test_y,y_pred)),str(precision_score(original_test_y,y_pred,average="macro")),str(recall_score(original_test_y,y_pred,average="macro")),str(f1_score(original_test_y,y_pred,average="macro")))						 
			



		report = classification_report(original_test_y, y_pred, output_dict=True)
		df = pd.DataFrame(report).transpose()
		print(df)
		print(classification_report(original_test_y, y_pred, output_dict=True))

	except Exception as exc:
		raise exc

i = 1
JMP_STEP = 1

while(PER_range_raw < 878):
	
	if(PER_range_raw < 878):
		print("lan loop thu ", i)
		i += 1
		print("PER_range_raw = ", PER_range_raw)
		#print("API_range_raw = ", API_range_raw)
		first = time.time()
		readData(JMP_STEP)
		second = time.time()
		print("Thoi gian chay: ", round(second - first, 2), "s")
	print("Acc so sánh: ", Acc_test)
	#if(Acc_test<0.955):
	#	print("giá trị trung bình nhỏ hơn ngưỡng cho phép")
	#	break
	#	print("khong chạy được tới đây")
	#else:
	#	print("giá trị vẫn lớn hơn ngưỡng nên vẫn chạy tiếp")
	PER_range_raw += JMP_STEP
	#API_range_raw += JMP_STEP

print("Done")

