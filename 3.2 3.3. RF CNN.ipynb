{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyoySJ0tTlz1"
      },
      "source": [
        "Import lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVXjEiLiTftt"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Flatten, Dropout\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kATif8N4TiTW"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPH8YR3HThvG"
      },
      "source": [
        "train_df = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/per_api_drebin_amd_benign_filter40/train-0.csv', header=None, skiprows=1)\n",
        "val_df = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/per_api_drebin_amd_benign_filter40/file-0.csv', header=None, skiprows=1)\n",
        "test_df = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/per_api_drebin_amd_benign_filter40/file-1.csv', header=None, skiprows=1)\n",
        "\n",
        "train_x = np.array(train_df.iloc[:, 3:])\n",
        "train_y = np.array(train_df.iloc[:, 2])\n",
        "\n",
        "val_x = np.array(val_df.iloc[:, 3:])\n",
        "val_y = np.array(val_df.iloc[:, 2])\n",
        "\n",
        "test_x = np.array(test_df.iloc[:, 3:])\n",
        "test_y = np.array(test_df.iloc[:, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQCwFg5HTp8Z"
      },
      "source": [
        "1CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghlNpmoiKmYu"
      },
      "source": [
        "def cnn_model(X_train, y_train, X_val, y_val):\n",
        "  '''Complie, train and get report of model CNN.\n",
        "  Input:\n",
        "    X_train: Feauters of train dataset\n",
        "    y_train: Labels of train dataset\n",
        "    X_val: Feauters of validate dataset\n",
        "    y_train: Labels of validate dataset\n",
        "  Ouput:\n",
        "    model: Model 1CNN\n",
        "  '''\n",
        "  BATCH_SIZE = 512\n",
        "  FEUTURES_SIZE = 44\n",
        "  PADDING_SIZE = 28\n",
        "  N_CLASSES = 228\n",
        "  LR = 0.001\n",
        "  N_EPOCHS = 18\n",
        "\n",
        "  #padding\n",
        "  train_x = np.concatenate((X_train, np.zeros((X_train.shape[0], PADDING_SIZE))),1)\n",
        "  val_x = np.concatenate((X_val, np.zeros((X_val.shape[0], PADDING_SIZE))),1)\n",
        "\n",
        "  #transform data\n",
        "  train_x_cnn = train_x.reshape(train_x.shape[0], FEUTURES_SIZE, FEUTURES_SIZE, 1)\n",
        "  val_x_cnn = val_x.reshape(val_x.shape[0], FEUTURES_SIZE, FEUTURES_SIZE, 1)\n",
        "\n",
        "  train_y_cnn = to_categorical(y_train, N_CLASSES)\n",
        "  val_y_cnn = to_categorical(y_val, N_CLASSES)\n",
        "\n",
        "  #complie model\n",
        "  input_cnn = Input(shape=(FEUTURES_SIZE, FEUTURES_SIZE,1))\n",
        "  conv1 = Conv2D(32, kernel_size=2, activation='relu', padding=\"same\", input_shape=(FEUTURES_SIZE, FEUTURES_SIZE, 1))(input_cnn)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "  conv2 = Conv2D(32, kernel_size=2, activation='relu', padding=\"same\")(pool1)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "  conv3 = Conv2D(64, kernel_size=2, activation='relu', padding=\"same\")(pool2)\n",
        "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "  dropout5 = Dropout(0.15)(pool3)\n",
        "  flatten_cnn = Flatten()(dropout5)\n",
        "\n",
        "  hidden1 = Dense(2048, activation='relu')(flatten_cnn)\n",
        "  hidden2 = Dense(1024, activation='relu')(hidden1)\n",
        "\n",
        "  dropout3 = Dropout(0.7)(hidden2)\n",
        "  output_cnn = Dense(N_CLASSES, activation='softmax')(dropout3)\n",
        "\n",
        "  model_cnn = Model(inputs=input_cnn, outputs=output_cnn, name=\"model_cnn\")\n",
        "  model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  #train\n",
        "  tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
        "  model_cnn.fit(train_x_cnn, train_y_cnn, epochs=N_EPOCHS, batch_size=BATCH_SIZE ,\n",
        "                      validation_data=(val_x_cnn, val_y_cnn), callbacks=[tensorBoardCallback])\n",
        "  \n",
        "  test_logits = model_cnn.predict(val_x_cnn)\n",
        "  #Lấy phần tử có giá trị lớn nhất\n",
        "  y_pred = np.argmax(test_logits, axis=-1)\n",
        "  #Kết quả thu được\n",
        "  rp = classification_report(y_val, y_pred, output_dict=True)\n",
        "  del rp['accuracy'], rp['macro avg'], rp['weighted avg']\n",
        "  weights = [0] * 228\n",
        "  for key in rp:\n",
        "    weights[int(key)] = float(rp[key]['precision']) \n",
        "  print(weights)\n",
        "  \n",
        "  return model_cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-BxZj8SXlCB",
        "outputId": "bd21baad-dbf6-414d-c54b-3ca65a8132a6"
      },
      "source": [
        "cnn = cnn_model(train_x, train_y, val_x, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/18\n",
            " 1/49 [..............................] - ETA: 0s - loss: 5.4426 - accuracy: 0.0020WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            " 2/49 [>.............................] - ETA: 1s - loss: 5.2150 - accuracy: 0.1133WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0176s vs `on_train_batch_end` time: 0.0464s). Check your callbacks.\n",
            "49/49 [==============================] - 1s 20ms/step - loss: 2.2995 - accuracy: 0.4454 - val_loss: 1.0962 - val_accuracy: 0.7165\n",
            "Epoch 2/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.8492 - accuracy: 0.7734 - val_loss: 0.5063 - val_accuracy: 0.8491\n",
            "Epoch 3/18\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.4920 - accuracy: 0.8600 - val_loss: 0.3238 - val_accuracy: 0.9009\n",
            "Epoch 4/18\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.3485 - accuracy: 0.8968 - val_loss: 0.2587 - val_accuracy: 0.9179\n",
            "Epoch 5/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2694 - accuracy: 0.9146 - val_loss: 0.2066 - val_accuracy: 0.9280\n",
            "Epoch 6/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2239 - accuracy: 0.9274 - val_loss: 0.1795 - val_accuracy: 0.9397\n",
            "Epoch 7/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.1855 - accuracy: 0.9352 - val_loss: 0.1694 - val_accuracy: 0.9368\n",
            "Epoch 8/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.1617 - accuracy: 0.9427 - val_loss: 0.1533 - val_accuracy: 0.9492\n",
            "Epoch 9/18\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.1458 - accuracy: 0.9475 - val_loss: 0.1670 - val_accuracy: 0.9397\n",
            "Epoch 10/18\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.1314 - accuracy: 0.9500 - val_loss: 0.1494 - val_accuracy: 0.9472\n",
            "Epoch 11/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.1165 - accuracy: 0.9562 - val_loss: 0.1404 - val_accuracy: 0.9492\n",
            "Epoch 12/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.1025 - accuracy: 0.9603 - val_loss: 0.1492 - val_accuracy: 0.9417\n",
            "Epoch 13/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.0956 - accuracy: 0.9634 - val_loss: 0.1424 - val_accuracy: 0.9534\n",
            "Epoch 14/18\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.0880 - accuracy: 0.9640 - val_loss: 0.1404 - val_accuracy: 0.9531\n",
            "Epoch 15/18\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.0822 - accuracy: 0.9666 - val_loss: 0.1399 - val_accuracy: 0.9537\n",
            "Epoch 16/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.0793 - accuracy: 0.9672 - val_loss: 0.1350 - val_accuracy: 0.9521\n",
            "Epoch 17/18\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.0719 - accuracy: 0.9689 - val_loss: 0.1390 - val_accuracy: 0.9505\n",
            "Epoch 18/18\n",
            "49/49 [==============================] - 1s 14ms/step - loss: 0.0699 - accuracy: 0.9699 - val_loss: 0.1462 - val_accuracy: 0.9518\n",
            "[0.997037037037037, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0.9375, 0, 0, 0, 0, 0, 0.7142857142857143, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.9669421487603306, 0, 0, 0, 0, 0, 0.3333333333333333, 0, 1.0, 0, 0.6883116883116883, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0.875, 0, 0.5, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0.75, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9838709677419355, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.3333333333333333, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9824561403508771, 1.0, 1.0, 0, 1.0, 0.95, 0, 0.919889502762431, 1.0, 0, 0, 0.8318965517241379, 0, 1.0, 0.7777777777777778, 0.0, 0.9333333333333333, 1.0, 1.0, 0.9658119658119658, 1.0, 0, 0.8846153846153846, 1.0, 0.9047619047619048, 0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 0.85, 1.0, 0.9333333333333333, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.975, 0, 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKJbYMPCZTVe"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbb8lcfpZSVX",
        "outputId": "af736861-2728-4227-cda5-65f10e82815c"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_rfc10 = RandomForestClassifier(max_depth=10)\n",
        "history_rfc10 = model_rfc10.fit(train_x, train_y)\n",
        "\n",
        "model_rfc50 = RandomForestClassifier(max_depth=50)\n",
        "history_rfc50 = model_rfc50.fit(train_x, train_y)\n",
        "\n",
        "model_rfc100 = RandomForestClassifier(max_depth=20)\n",
        "history_rfc100 = model_rfc100.fit(train_x, train_y)\n",
        "\n",
        "y_pred = history_rfc10.predict(val_x)\n",
        "rp = classification_report(val_y, y_pred, output_dict=True)\n",
        "del rp['accuracy'], rp['macro avg'], rp['weighted avg']\n",
        "weights = [0] * 228\n",
        "for key in rp:\n",
        "  weights[int(key)] = float(rp[key]['precision'])\n",
        "print(weights)\n",
        "  \n",
        "\n",
        "\n",
        "y_pred = history_rfc50.predict(val_x)\n",
        "rp = classification_report(val_y, y_pred, output_dict=True)\n",
        "del rp['accuracy'], rp['macro avg'], rp['weighted avg']\n",
        "weights = [0] * 228\n",
        "for key in rp:\n",
        "  weights[int(key)] = float(rp[key]['precision'])\n",
        "print(weights)\n",
        "\n",
        "\n",
        "\n",
        "y_pred = history_rfc100.predict(val_x)\n",
        "rp = classification_report(val_y, y_pred, output_dict=True)\n",
        "del rp['accuracy'], rp['macro avg'], rp['weighted avg']\n",
        "weights = [0] * 228\n",
        "for key in rp:\n",
        "  weights[int(key)] = float(rp[key]['precision'])\n",
        "print(weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.9768451519536903, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.7142857142857143, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.9831932773109243, 0, 0, 0, 0, 0, 0.0, 0, 1.0, 0, 0.9666666666666667, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0.8, 0, 0.0, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.6666666666666666, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9567307692307693, 1.0, 1.0, 0, 1.0, 1.0, 0, 0.8067632850241546, 1.0, 0, 0, 0.7678571428571429, 0, 1.0, 1.0, 0.0, 1.0, 0.9464285714285714, 1.0, 0.9140625, 1.0, 0, 0.8260869565217391, 1.0, 1.0, 0, 1.0, 1.0, 0.0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9253731343283582, 0, 0]\n",
            "[0.9897058823529412, 0, 0, 0, 0.75, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.9915254237288136, 0, 0, 0, 0, 0, 0.3333333333333333, 0, 1.0, 0, 0.691358024691358, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0.84375, 0, 0.16666666666666666, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9838709677419355, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.5714285714285714, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9754299754299754, 1.0, 1.0, 0, 1.0, 1.0, 0, 0.8862433862433863, 1.0, 0, 0, 0.8464912280701754, 0, 1.0, 0.7, 0.0, 1.0, 0.9310344827586207, 1.0, 0.9914529914529915, 0.9411764705882353, 0, 0.8571428571428571, 1.0, 1.0, 0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9624060150375939, 0, 0]\n",
            "[0.9911894273127754, 0, 0, 0, 0.8181818181818182, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.6666666666666666, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.9915254237288136, 0, 0, 0, 0, 0, 0.5, 0, 1.0, 0, 0.691358024691358, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0.7714285714285715, 0, 0.16666666666666666, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9778325123152709, 1.0, 1.0, 0, 1.0, 1.0, 0, 0.8769633507853403, 1.0, 0, 0, 0.8464912280701754, 0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.9482758620689655, 1.0, 0.9831932773109243, 1.0, 0, 0.88, 1.0, 1.0, 0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9618320610687023, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPLPABf1bKcA"
      },
      "source": [
        "RFC_WEIGHTS10 = [0.974025974025974, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.6666666666666666, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.9915254237288136, 0, 0, 0, 0, 0, 0.0, 0, 1.0, 0, 0.9666666666666667, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0.7777777777777778, 0, 0.0, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9838709677419355, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9543269230769231, 1.0, 1.0, 0, 1.0, 1.0, 0, 0.8141809290953546, 1.0, 0, 0, 0.7706093189964157, 0, 1.0, 1.0, 0.0, 1.0, 0.9122807017543859, 1.0, 0.928, 1.0, 0, 0.8260869565217391, 1.0, 1.0, 0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 1.0, 0.9444444444444444, 1.0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9057971014492754, 0, 0]\n",
        "RFC_WEIGHTS50 = [0.9897360703812317, 0, 0, 0, 0.8, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.6666666666666666, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.9915254237288136, 0, 0, 0, 0, 0, 0.3333333333333333, 0, 1.0, 0, 0.6842105263157895, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0.7941176470588235, 0, 0.16666666666666666, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9778325123152709, 1.0, 1.0, 0, 1.0, 1.0, 0, 0.8723958333333334, 1.0, 0, 0, 0.8318965517241379, 0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.9473684210526315, 1.0, 0.975, 1.0, 0, 0.8214285714285714, 1.0, 1.0, 0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9618320610687023, 0, 0]\n",
        "RFC_WEIGHTS100 = [0.9897360703812317, 0, 0, 0, 0.8, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.6666666666666666, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.9915254237288136, 0, 0, 0, 0, 0, 0.3333333333333333, 0, 1.0, 0, 0.6842105263157895, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0.7941176470588235, 0, 0.16666666666666666, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9778325123152709, 1.0, 1.0, 0, 1.0, 1.0, 0, 0.8723958333333334, 1.0, 0, 0, 0.8318965517241379, 0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.9473684210526315, 1.0, 0.975, 1.0, 0, 0.8214285714285714, 1.0, 1.0, 0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9618320610687023, 0, 0]\n",
        "CNN_WEIGHTS = [0.997037037037037, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0.9375, 0, 0, 0, 0, 0, 0.7142857142857143, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.9669421487603306, 0, 0, 0, 0, 0, 0.3333333333333333, 0, 1.0, 0, 0.6883116883116883, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0.875, 0, 0.5, 0, 0, 0, 0, 0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0.75, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.9838709677419355, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.3333333333333333, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9824561403508771, 1.0, 1.0, 0, 1.0, 0.95, 0, 0.919889502762431, 1.0, 0, 0, 0.8318965517241379, 0, 1.0, 0.7777777777777778, 0.0, 0.9333333333333333, 1.0, 1.0, 0.9658119658119658, 1.0, 0, 0.8846153846153846, 1.0, 0.9047619047619048, 0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 0.85, 1.0, 0.9333333333333333, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0.975, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V1LsR1ctW7S"
      },
      "source": [
        "class CustomSoftVotingClassifier:\n",
        "  def __init__(self, models=[], weights=[]):\n",
        "    self.models = models\n",
        "    self.weights = weights\n",
        "\n",
        "  def __check(self):\n",
        "    if (len(self.models) != len(self.weights)):\n",
        "      return False\n",
        "    return True\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    if self.__check():\n",
        "      output = []\n",
        "      lbs1 = history_rfc10.predict(X_test)\n",
        "      print(\"RFC10: \", accuracy_score(lbs1, test_y))\n",
        "      lbs2 = history_rfc50.predict(X_test)\n",
        "      print(\"RFC50: \", accuracy_score(lbs2, test_y))\n",
        "\n",
        "      lbs3 = history_rfc100.predict(X_test)\n",
        "      print(\"RFC100: \", accuracy_score(lbs3, test_y))\n",
        "      \n",
        "      # X_test = np.concatenate((X_test, np.zeros((X_test.shape[0], 28))),1)\n",
        "      # X_test = X_test.reshape(X_test.shape[0], 44, 44, 1)\n",
        "      # lbs3 = np.argmax(cnn.predict(X_test), axis=-1)\n",
        "      # print(\"CNN: \", accuracy_score(lbs3, test_y))\n",
        "      \n",
        "      tmp1 = [self.weights[1][lb] for lb in lbs1]\n",
        "      tmp2 = [self.weights[2][lb] for lb in lbs2]\n",
        "      tmp3 = [self.weights[0][lb] for lb in lbs3]\n",
        "      \n",
        "      for i in range(len(tmp1)):\n",
        "        if tmp1[i] == tmp2[i] == tmp3[i]:\n",
        "          tmp = [lbs1[i], lbs2[i], lbs3[i]]\n",
        "          if tmp.count(lbs1[i]) >= 2:\n",
        "            output.append(lbs1[i])\n",
        "          elif tmp.count(lbs2[i]) >= 2:\n",
        "            output.append(lbs2[i])\n",
        "          else:\n",
        "            output.append(lbs3[i])\n",
        "        else:\n",
        "          m = max(tmp1[i], tmp2[i], tmp3[i])\n",
        "          if m == tmp3[i]:\n",
        "            output.append(lbs3[i])\n",
        "          elif m == tmp1[i]:\n",
        "            output.append(lbs1[i])\n",
        "          else:\n",
        "            output.append(lbs2[i])\n",
        "\n",
        "      return output\n",
        "    else:\n",
        "      print(\"len(models) != len(weights)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVQptAXcwEyq",
        "outputId": "36122eac-4c7e-446e-aabf-2fe8a42b31e3"
      },
      "source": [
        "# csvc = CustomSoftVotingClassifier([cnn, history_rfc10, history_rfc50], [CNN_WEIGHTS, RFC_WEIGHTS10, RFC_WEIGHTS50])\n",
        "csvc = CustomSoftVotingClassifier([history_rfc100, history_rfc10, history_rfc50], [RFC_WEIGHTS100, RFC_WEIGHTS10, RFC_WEIGHTS50])\n",
        "y_pred = csvc.predict(test_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RFC10:  0.9244053437601825\n",
            "RFC50:  0.946236559139785\n",
            "RFC100:  0.9475399152818508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVuzFFY3tDGq",
        "outputId": "76cc4829-58f4-4e4c-f726-cc291d42526e"
      },
      "source": [
        "report = classification_report(test_y, y_pred)\n",
        "print(report)\n",
        "print(accuracy_score(y_pred, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       677\n",
            "           4       0.82      1.00      0.90         9\n",
            "          12       1.00      0.91      0.95        32\n",
            "          18       0.67      0.57      0.62         7\n",
            "          32       1.00      1.00      1.00         8\n",
            "          33       0.99      0.97      0.98       121\n",
            "          39       0.50      0.50      0.50         6\n",
            "          41       1.00      1.00      1.00        15\n",
            "          43       0.69      0.61      0.65        92\n",
            "          48       1.00      1.00      1.00         6\n",
            "          64       1.00      1.00      1.00         5\n",
            "          69       0.77      0.82      0.79        33\n",
            "          71       0.17      0.17      0.17         6\n",
            "          77       1.00      1.00      1.00        15\n",
            "          78       1.00      1.00      1.00         4\n",
            "          85       1.00      1.00      1.00        14\n",
            "          87       1.00      0.50      0.67         4\n",
            "         104       1.00      1.00      1.00         8\n",
            "         110       1.00      0.98      0.99        62\n",
            "         117       1.00      0.97      0.98        62\n",
            "         133       1.00      1.00      1.00         5\n",
            "         139       0.50      0.50      0.50         4\n",
            "         180       0.98      1.00      0.99       398\n",
            "         181       1.00      0.75      0.86         4\n",
            "         182       1.00      0.50      0.67         4\n",
            "         184       1.00      1.00      1.00        64\n",
            "         185       1.00      0.38      0.55        21\n",
            "         187       0.88      1.00      0.93       336\n",
            "         188       1.00      1.00      1.00         4\n",
            "         191       0.85      0.90      0.87       215\n",
            "         193       1.00      1.00      1.00       126\n",
            "         194       0.67      0.33      0.44        12\n",
            "         195       0.00      0.00      0.00         5\n",
            "         196       1.00      0.86      0.92        14\n",
            "         197       0.95      1.00      0.97        55\n",
            "         198       1.00      0.83      0.91         6\n",
            "         199       0.98      0.98      0.98       119\n",
            "         200       1.00      0.94      0.97        17\n",
            "         202       0.88      0.69      0.77        32\n",
            "         203       1.00      1.00      1.00       181\n",
            "         204       1.00      0.90      0.95        20\n",
            "         206       1.00      1.00      1.00         8\n",
            "         207       1.00      0.48      0.65        23\n",
            "         208       1.00      0.50      0.67         6\n",
            "         212       1.00      1.00      1.00         4\n",
            "         213       1.00      0.88      0.94        17\n",
            "         214       1.00      1.00      1.00        17\n",
            "         215       1.00      0.81      0.90        16\n",
            "         219       1.00      0.95      0.98        21\n",
            "         225       0.96      0.98      0.97       129\n",
            "\n",
            "    accuracy                           0.95      3069\n",
            "   macro avg       0.90      0.82      0.85      3069\n",
            "weighted avg       0.95      0.95      0.94      3069\n",
            "\n",
            "0.9465623981753014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gcv0khc9j9N",
        "outputId": "b9c592cf-d7c6-41a9-d567-b939757e7f29"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "eclf3 = VotingClassifier(estimators=[('lr', model_rfc10), ('rf', model_rfc50), ('gnb', model_rfc100)], voting='hard', weights=[1,1,1], flatten_transform=True)\n",
        "eclf3 = eclf3.fit(train_x, train_y)\n",
        "y_pred = eclf3.predict(test_x)\n",
        "print(accuracy_score(y_pred, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9472140762463344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFYuBs9w0rAZ",
        "outputId": "8975e048-83ed-426d-df5f-72a7826decc7"
      },
      "source": [
        "print(y_pred)\n",
        "print(list(test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 187, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 180, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 180, 181, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 180, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 187, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 18, 18, 18, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 41, 47, 47, 49, 215, 87, 87, 187, 104, 104, 104, 104, 104, 104, 104, 196, 112, 148, 155, 155, 157, 180, 171, 171, 173, 173, 178, 178, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 204, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 204, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 0, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 181, 181, 181, 181, 182, 182, 182, 182, 183, 183, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 187, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 187, 187, 187, 187, 185, 187, 0, 185, 187, 180, 185, 0, 185, 187, 185, 185, 187, 187, 187, 185, 180, 186, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 0, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 0, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 0, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 204, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 180, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 33, 187, 187, 187, 187, 187, 187, 187, 187, 187, 0, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 0, 187, 187, 199, 187, 187, 225, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 199, 180, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 180, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 199, 187, 187, 187, 187, 202, 187, 187, 187, 0, 187, 187, 187, 187, 187, 187, 187, 199, 187, 187, 187, 187, 187, 187, 187, 187, 180, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 0, 187, 187, 188, 188, 188, 188, 187, 190, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 0, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 199, 195, 195, 195, 195, 195, 196, 180, 196, 196, 196, 196, 196, 196, 196, 196, 196, 0, 196, 196, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 187, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 0, 197, 197, 197, 197, 197, 197, 197, 197, 0, 197, 197, 198, 198, 198, 198, 198, 198, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 187, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 187, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 180, 200, 225, 200, 200, 187, 201, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 207, 0, 202, 0, 202, 202, 0, 202, 202, 0, 0, 33, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 204, 204, 204, 204, 204, 204, 204, 204, 180, 204, 204, 204, 204, 204, 204, 204, 180, 180, 204, 204, 187, 206, 207, 225, 207, 207, 207, 207, 207, 104, 207, 207, 207, 207, 207, 207, 207, 225, 207, 207, 225, 207, 207, 207, 207, 208, 208, 208, 208, 87, 208, 209, 210, 187, 212, 212, 212, 212, 197, 213, 213, 213, 213, 213, 213, 213, 213, 213, 213, 197, 213, 213, 213, 213, 213, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 215, 215, 215, 215, 215, 225, 215, 215, 215, 215, 180, 215, 180, 215, 215, 180, 216, 217, 218, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 220, 221, 187, 223, 224, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 0, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 187, 187, 227]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 18, 18, 18, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 41, 47, 47, 49, 56, 87, 87, 87, 104, 104, 104, 104, 104, 104, 104, 110, 112, 148, 155, 155, 157, 161, 171, 171, 173, 173, 178, 178, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 181, 181, 181, 181, 182, 182, 182, 182, 183, 183, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 184, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 185, 186, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 187, 188, 188, 188, 188, 189, 190, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 191, 192, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 193, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 196, 196, 196, 196, 196, 196, 196, 196, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 197, 198, 198, 198, 198, 198, 198, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 199, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 201, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 203, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 205, 206, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 208, 208, 208, 208, 208, 208, 209, 210, 211, 212, 212, 212, 212, 213, 213, 213, 213, 213, 213, 213, 213, 213, 213, 213, 213, 213, 213, 213, 213, 213, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 214, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 216, 217, 218, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 219, 220, 221, 222, 223, 224, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 226, 226, 227]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}