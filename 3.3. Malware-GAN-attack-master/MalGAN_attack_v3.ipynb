{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers.merge import Maximum, Concatenate\n",
    "# from keras.layers import Maximum, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution() # work with tensorflow 2.0\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from model import Model as Classifier\n",
    "import pickle\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style = \"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data is a list of 3435 malware examples, each with 3514 API features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dict = pickle.load(open('feat_dict.pickle', 'rb'), encoding='latin1')\n",
    "features = []\n",
    "sha1 = []\n",
    "for key in seed_dict:\n",
    "    seed_dict[key] = seed_dict[key].toarray()[0]\n",
    "    features.append(seed_dict[key])\n",
    "    sha1.append(key)\n",
    "feed_feat = np.stack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset contains 13190 samples, each with 3514 API features. The first 6896 are malware example, and the last 6294 are benign examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalGAN():\n",
    "    def __init__(self, model_name):\n",
    "        self.apifeature_dims = 3514\n",
    "        self.z_dims = 100   # noise appended at the end of example\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.hide_layers = 256\n",
    "        self.generator_layers = [self.apifeature_dims+self.z_dims, self.hide_layers, self.apifeature_dims]\n",
    "        self.substitute_detector_layers = [self.apifeature_dims, self.hide_layers, 1]\n",
    "        self.blackbox, self.sess = self.build_blackbox_detector(self.model_name)\n",
    "        self.optimizer = Adam(lr=0.001)\n",
    "        \n",
    "        # Build and compile the substitute_detector\n",
    "        self.substitute_detector = self.build_substitute_detector()\n",
    "        self.substitute_detector.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes malware and noise as input and generates adversarial malware examples\n",
    "        example = Input(shape=(self.apifeature_dims,))\n",
    "        noise = Input(shape=(self.z_dims,))\n",
    "        input = [example, noise]\n",
    "        malware_examples = self.generator(input)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.substitute_detector.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.substitute_detector(malware_examples)\n",
    "\n",
    "        # The combined model  (stacked generator and substitute_detector)\n",
    "        self.combined = Model(input, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
    "        \n",
    "        \n",
    "    def build_generator(self):\n",
    "\n",
    "        example = Input(shape=(self.apifeature_dims,))\n",
    "        noise = Input(shape=(self.z_dims,))\n",
    "        x = Concatenate(axis=1)([example, noise])\n",
    "        for dim in self.generator_layers[1:]:\n",
    "            x = Dense(dim)(x)\n",
    "            x = Activation(activation='sigmoid')(x)\n",
    "        x = Maximum()([example, x])\n",
    "        generator = Model([example, noise], x, name='generator')\n",
    "        generator.summary()\n",
    "        return generator\n",
    "    \n",
    "    def build_substitute_detector(self):\n",
    "\n",
    "        input = Input(shape=(self.substitute_detector_layers[0],))\n",
    "        x = input\n",
    "        for dim in self.substitute_detector_layers[1:]:\n",
    "            x = Dense(dim)(x)\n",
    "            x = Activation(activation='sigmoid')(x)\n",
    "        substitute_detector = Model(input, x, name='substitute_detector')\n",
    "        substitute_detector.summary()\n",
    "        return substitute_detector    \n",
    "    \n",
    "\n",
    "    def build_blackbox_detector(self,model_name):\n",
    "        \n",
    "        PATH = \"adv_trained/{}.ckpt\".format(model_name)\n",
    "        # Clear the current graph in each run, to avoid variable duplication\n",
    "        tf.reset_default_graph()\n",
    "        model = Classifier()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "                    \n",
    "        saver.restore(sess, PATH)\n",
    "        print (\"load model from:\", PATH)\n",
    "        \n",
    "        return model, sess\n",
    "        \n",
    "    def train(self, epochs, batch_size):\n",
    "        \n",
    "        model = self.blackbox\n",
    "        sess = self.sess\n",
    "        \n",
    "                \n",
    "        # Load test dataset (all malware)\n",
    "        seed_dict = pickle.load(open('feat_dict.pickle', 'rb'), encoding='latin1')\n",
    "        features = []\n",
    "        sha1 = []\n",
    "        dist_dict = {} # [key]: hash [value]: L0 distance\n",
    "        for key in seed_dict:\n",
    "            seed_dict[key] = seed_dict[key].toarray()[0]\n",
    "            features.append(seed_dict[key])\n",
    "            sha1.append(key)\n",
    "        feed_feat = np.stack(features)\n",
    "        xtest_mal, ytest_mal = feed_feat, np.ones(len(feed_feat))\n",
    "        \n",
    "\n",
    "        # Load training dataset\n",
    "        train_x, train_y = load_svmlight_file(\"train_data.libsvm\",\n",
    "                                       n_features=3514,\n",
    "                                       multilabel=False, \n",
    "                                       zero_based=False,\n",
    "                                       query_id=False)\n",
    "        \n",
    "        train_x = train_x.toarray()\n",
    "        xtrain_ben = train_x[6896:]\n",
    "        ytrain_ben = train_y[6896:]\n",
    "        xtrain_mal = train_x[0:6896]              \n",
    "        ytrain_mal = train_y[0:6896]\n",
    "        \n",
    "        # Since the training dataset is unbalanced, we randomly choose sample from benign dataset\n",
    "        # and add them to the end to make up the gap\n",
    "        idx = np.random.randint(0, xtrain_ben.shape[0], 6896 - 6294)\n",
    "        add_on = xtrain_ben[idx]\n",
    "        add_on_label = ytrain_ben[idx]\n",
    "        xtrain_ben = np.concatenate((xtrain_ben, add_on), axis=0)\n",
    "        ytrain_ben = np.concatenate((ytrain_ben, add_on_label), axis = 0)\n",
    "\n",
    "        Test_TPR = []\n",
    "        d_loss_list, g_loss_list = [], []\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Each epoch goes through all the data in the training set\n",
    "            start = 0                \n",
    "                \n",
    "            for step in range(xtrain_mal.shape[0] // batch_size):\n",
    "                \n",
    "                # ---------------------\n",
    "                #  Train substitute_detector\n",
    "                # ---------------------\n",
    "\n",
    "                xmal_batch = xtrain_mal[start : start + batch_size]  \n",
    "                noise = np.random.uniform(0, 1, (batch_size, self.z_dims))\n",
    "\n",
    "                xben_batch = xtrain_ben[start : start + batch_size]\n",
    "                start = start + batch_size\n",
    "                \n",
    "                # predict using blackbox detector              \n",
    "                yben_batch = sess.run(model.y_pred,\\\n",
    "                    feed_dict={model.x_input:xben_batch})\n",
    "\n",
    "                # Generate a batch of new malware examples\n",
    "                gen_examples = self.generator.predict([xmal_batch, noise])\n",
    "                ymal_batch = sess.run(model.y_pred,\\\n",
    "                                      feed_dict={model.x_input:np.ones(gen_examples.shape)*(gen_examples > 0.5)})\n",
    "                \n",
    "                # Train the substitute_detector\n",
    "                d_loss_real = self.substitute_detector.train_on_batch(xben_batch, yben_batch)\n",
    "                d_loss_fake = self.substitute_detector.train_on_batch(gen_examples, ymal_batch)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                noise = np.random.uniform(0, 1, (batch_size, self.z_dims))\n",
    "                g_loss = self.combined.train_on_batch([xmal_batch,noise], np.zeros((batch_size, 1)))\n",
    "\n",
    "            \n",
    "            # After each epoch, Evaluate evasion performance on the test dataset\n",
    "            # try different noise for 3 times\n",
    "            for j in range(3):\n",
    "                noise = np.random.uniform(0, 1, (xtest_mal.shape[0], self.z_dims))\n",
    "                gen_examples = self.generator.predict([xtest_mal, noise])\n",
    "                    \n",
    "                TPR = sess.run(model.accuracy,\\\n",
    "                        feed_dict={model.x_input:np.ones(gen_examples.shape)*(gen_examples > 0.5), model.y_input: np.ones(gen_examples.shape[0],)})\n",
    "            \n",
    "                Test_TPR.append(TPR)\n",
    "            \n",
    "                transformed_to_bin = np.ones(gen_examples.shape)*(gen_examples > 0.5)\n",
    "            \n",
    "                pred_y_label = sess.run(model.y_pred,\\\n",
    "                                     feed_dict={model.x_input:np.ones(gen_examples.shape)*(gen_examples > 0.5)})\n",
    "            \n",
    "            \n",
    "            \n",
    "                # remove successfully evaded malware examples from xtest_mal\n",
    "                i = 0\n",
    "                while i  < pred_y_label.shape[0]:\n",
    "                    if pred_y_label[i] == 0: # should be 1 but predict 0\n",
    "                    #print(sha1[i], xtrain_mal[i])\n",
    "                    # calculate L0 distance and put to dictionary\n",
    "                        L0 = np.sum(transformed_to_bin[i]) - np.sum(xtest_mal[i]) #insertion only\n",
    "                        dist_dict[sha1[i]] = L0  # [key]: hash [value]: L0 distance\n",
    "                        xtest_mal = np.delete(xtest_mal, i, 0)\n",
    "                        pred_y_label = np.delete(pred_y_label, i, 0)\n",
    "                        sha1 = sha1[:i] + sha1[i+1:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "            \n",
    "                print(\"remaining malware examples:\", xtest_mal.shape[0])\n",
    "                if xtest_mal.shape[0] == 0:\n",
    "                    break #successful evade all\n",
    "\n",
    "        \n",
    "            # Print and record the progress\n",
    "            print(\"[MalGAN] epoch(%d) [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch+1, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            print(\"[Classifier] Test TPR on remaining test data: %f\" % (Test_TPR[-1]))\n",
    "            d_loss_list.append(d_loss[0])\n",
    "            g_loss_list.append(g_loss)\n",
    "            if xtest_mal.shape[0] == 0:\n",
    "                break #successful evade all\n",
    "            \n",
    "\n",
    "        sess.close()\n",
    "        \n",
    "        # AFTER all epochs\n",
    "        # Plot the progress --> loss\n",
    "        # d_loss_df = pd.DataFrame(dict(epoch =np.arange(1, len(d_loss_list)+1),\\\n",
    "        #                               dataset = \"d loss\",\\\n",
    "        #                               loss = np.asarray(d_loss_list, dtype=np.float32)))\n",
    "        # g_loss_df = pd.DataFrame(dict(epoch =np.arange(1, len(g_loss_list)+1),\\\n",
    "        #                               dataset = \"g loss\",\\\n",
    "        #                               loss = np.asarray(g_loss_list, dtype=np.float32)))\n",
    "\n",
    "        # loss_df = pd.concat([d_loss_df,g_loss_df], axis=0).reset_index(drop=True)      \n",
    "        # plt.figure()\n",
    "        # loss_plot = sns.lineplot(x = 'epoch', y = 'loss', hue ='dataset', data = loss_df)\n",
    "        # handles, labels = loss_plot.get_legend_handles_labels()\n",
    "        # loss_plot.legend(handles=handles, labels=labels)\n",
    "        # plt.show()\n",
    "        # fig = loss_plot.get_figure()\n",
    "        # fig.savefig(\"{}_loss\".format(self.model_name))  # save loss plot\n",
    "        \n",
    "        # get corresponding dataframe for different models\n",
    "        ERA = []\n",
    "        success_num = 0\n",
    "        # Calculate ERA for each L0 distance\n",
    "        for i in range(3515): # 0 - 3514 features\n",
    "            for key in dist_dict:\n",
    "                if dist_dict[key] == i:\n",
    "                    success_num += 1\n",
    "            ERA.append((3435 - success_num) / 3435)\n",
    "        \n",
    "        # report ERA if not completely evaded\n",
    "        if ERA[-1] != 0:\n",
    "            print(\"{} is not completely evaded after {} epochs. ERA = {}\".format(self.model_name, epochs, ERA[-1]))\n",
    "            \n",
    "            \n",
    "        if self.model_name == 'baseline_checkpoint': curve_name = 'Baseline'\n",
    "        if self.model_name == \"baseline_adv_delete_one\": curve_name = 'Adv Retrain A'\n",
    "        if self.model_name == \"robust_delete_one\": curve_name = 'Robust A' \n",
    "        if self.model_name == \"baseline_adv_insert_one\": curve_name = 'Adv Retrain B'\n",
    "        if self.model_name == \"robust_insert_one\": curve_name = 'Robust B'\n",
    "        if self.model_name == \"baseline_adv_delete_two\": curve_name = 'Adv Retrain C'\n",
    "        if self.model_name == \"robust_delete_two\": curve_name = 'Robust C'\n",
    "        if self.model_name == \"baseline_adv_insert_rootallbutone\": curve_name = 'Adv Retrain D'\n",
    "        if self.model_name == \"adv_keep_twocls\": curve_name = 'Ensemble D Base Learner'\n",
    "        if self.model_name == \"robust_monotonic\": curve_name = 'Robust E'\n",
    "        if self.model_name == \"baseline_adv_combine_two\": curve_name = 'Adv Retrain A+B'\n",
    "        if self.model_name == \"adv_del_twocls\": curve_name = 'Ensemble A+B Base Learner'\n",
    "        if self.model_name == \"robust_combine_two_v2_e18\": curve_name = 'Robust A+B'\n",
    "        if self.model_name == \"robust_insert_allbutone\": curve_name = 'Robust D'\n",
    "        if self.model_name == \"robust_combine_three_e17\": curve_name = 'Robust A+B+E'\n",
    "\n",
    "        model_df = pd.DataFrame(dict(ERA = np.asarray(ERA, dtype=np.float32),\\\n",
    "                                     model = curve_name, L0 = np.arange(3515)))\n",
    "        return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train MalGAN and plot result\n",
    "\n",
    "# models = ['adv_keep_twocls', 'adv_del_twocls']\n",
    "#models = ['baseline_adv_delete_one', 'baseline_adv_insert_one', 'baseline_adv_delete_two', \\\n",
    "          #'baseline_adv_insert_rootallbutone', 'baseline_adv_combine_two']\n",
    "models = ['baseline_checkpoint', 'robust_delete_one', 'robust_insert_one', 'robust_delete_two', \\\n",
    "          'robust_insert_allbutone', 'robust_monotonic', 'robust_combine_two_v2_e18', 'robust_combine_three_e17']\n",
    "\n",
    "dataframes = []\n",
    "for model in models:\n",
    "    malgan = MalGAN(model)\n",
    "    df = malgan.train(epochs = 50, batch_size= 128)\n",
    "    dataframes.append(df)\n",
    "\n",
    "data = pd.concat(dataframes, axis = 0).reset_index(drop=True)\n",
    "plt.figure()\n",
    "\n",
    "g = sns.lineplot(x = 'L0', y = 'ERA', data=data, hue='model')\n",
    "plt.xlabel(\"$L_0$\")\n",
    "g.set(yticks = [0.00, 0.25, 0.50, 0.75, 1.00])\n",
    "g.xaxis.set_major_locator(ticker.FixedLocator([10, 200, 500, 1000, 2000, 3514]))\n",
    "handles, labels = g.get_legend_handles_labels()\n",
    "g.legend(handles=handles, labels=labels)\n",
    "fig = g.get_figure()\n",
    "fig.savefig(\"result.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
